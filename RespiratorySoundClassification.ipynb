{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pw3IVv3cz38W"
   },
   "source": [
    "# Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHM7sVQd8h_X",
    "outputId": "00ad2327-8757-4faf-8bab-9e5c1927538d"
   },
   "outputs": [],
   "source": [
    "!pip install cmapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6FqFAF3wrnW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "import cmapy\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import utils\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import librosa.display\n",
    "from utils import *\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnujnOlcaXRc"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny86KDc5EC4i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "import cmapy\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import utils\n",
    "from torch.utils.data import random_split\n",
    "import torchaudio\n",
    "\n",
    "def get_annotation_data(file_name,data_dir): # Function returns the recording_data and the annotation_data\n",
    "  '''\n",
    "  Parameters\n",
    "  ----------\n",
    "  file_name: the file name of which data is to be retrived\n",
    "  data_dir: Directory where file is present\n",
    "  '''\n",
    "  annotation_data = pd.read_csv(os.path.join(data_dir,file_name+\".txt\"),sep=\"\\t\")\n",
    "  annotation_data.columns = [\"start\",\"end\",\"crackle\",\"wheeze\"]\n",
    "  file_data = file_name.split(\"_\")\n",
    "  recording_data = pd.DataFrame([file_data],columns = [\"pid\",\"recording_index\",\"chest_location\",\"acquisition_mode\",\"equipment\"])\n",
    "  return recording_data,annotation_data \n",
    "\n",
    "\n",
    "def save_images(image, train_flag):\n",
    "    save_dir = 'dump_image'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    if train_flag:\n",
    "        save_dir = os.path.join(save_dir, 'train')\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        cv2.imwrite(os.path.join(save_dir, image[1]+'_'+str(image[2])+'_'+str(image[3])+'_'+str(image[4])+'.jpg'), cv2.cvtColor(image[0], cv2.COLOR_RGB2BGR))\n",
    "    else:\n",
    "        save_dir = os.path.join(save_dir, 'test')\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        cv2.imwrite(os.path.join(save_dir, image[1]+'_'+str(image[2])+'_'+str(image[3])+'_'+str(image[4])+'.jpg'), cv2.cvtColor(image[0], cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def get_label(crackle, wheeze):\n",
    "    if crackle == 0 and wheeze == 0:\n",
    "        return 0\n",
    "    elif crackle == 1 and wheeze == 0:\n",
    "        return 1\n",
    "    elif crackle == 0 and wheeze == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def get_samples(file_name,dataset_dir,annotation_data,sample_rate):\n",
    "  samples = []\n",
    "  audio_file,original_sr = torchaudio.load(os.path.join(dataset_dir,file_name+\".wav\"))\n",
    "  audio_file = torchaudio.transforms.Resample(original_sr, sample_rate)(audio_file)\n",
    "#   print(\"Audio Chunk shape: \",audio_file.shape)\n",
    "#   print(\"Annotation Data: \",annotation_data)\n",
    "  for i in range(len(annotation_data.index)):\n",
    "        row = annotation_data.loc[i]\n",
    "        # print(\"Row: \",row)\n",
    "        start = row['start']\n",
    "        # print(\"Start: \",start)\n",
    "        end = row['end']\n",
    "        crackles = row['crackle']\n",
    "        wheezes = row['wheeze']\n",
    "        max_ind = audio_file.shape[1]\n",
    "        # print(\"Max Ind\",max_ind) \n",
    "        # split signal\n",
    "        start_ind = min(int(start * sample_rate), max_ind)\n",
    "        end_ind = min(int(end * sample_rate), max_ind)\n",
    "        # print(\"Start Ind\",start_ind,\"End Ind\",end_ind)\n",
    "        audio_chunk = audio_file[:,start_ind:end_ind]\n",
    "        # print(\"Audio Chunk\",audio_chunk.shape)\n",
    "        samples.append((audio_chunk, get_label(crackles, wheezes), start,end))\n",
    "  return samples\n",
    "\n",
    "def get_train_test_names(train_test_file_names):\n",
    "    '''\n",
    "    Get List of file names belonging in train and test datasets\n",
    "    ---------\n",
    "    train_test_file_names:  txt file containing names of all samples in train\n",
    "                            and test as given by ICBHI\n",
    "    return: train_names,test_names\n",
    "    '''\n",
    "    train_test = pd.read_csv(train_test_file_names,sep=\"\\t\",header=None)\n",
    "    train_names = train_test[train_test[1]==\"train\"]\n",
    "    test_names = train_test[train_test[1]==\"test\"]\n",
    "    return train_names,test_names\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fea1mTIs9qWO"
   },
   "source": [
    "# Implementing Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQN_jxC699TL"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1bkAGsF9VpI"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import librosa.display\n",
    "import torchaudio\n",
    "from utils import *\n",
    "\n",
    "class ImageLoader(Dataset):\n",
    "  def __init__(self,dataset_file,dataset_dir,train_flag=True,transform=None):\n",
    "    self.input_transform = transform\n",
    "    self.dataset_file = dataset_file\n",
    "    self.dataset_dir = dataset_dir\n",
    "    # self.dataset = pd.read_csv(os.path.join(dataset_dir,dataset_file),sep=\"\\t\",header=None)\n",
    "    self.dataset = dataset_file\n",
    "    self.audio_data = []\n",
    "    # self.train_flag = False\n",
    "    # Spectrogram parameters\n",
    "    self.train_flag = train_flag\n",
    "    self.sample_rate = 4000\n",
    "    self.desired_length = 8\n",
    "    self.n_mels = 128 #128\n",
    "    self.nfft = 1024 #2048\n",
    "    self.win_length = int(60/1000*self.sample_rate)\n",
    "    self.hop = self.win_length//2\n",
    "    self.f_max = 2000\n",
    "    self.device_to_files = []  # mapping the filename to device\n",
    "    # self.patient_to_device = {}\n",
    "    self.patient_to_samples ={}\n",
    "    self.patient_to_idx = {}\n",
    "\n",
    "    # files = os.listdir(dataset_dir)\n",
    "    failed_files=[]\n",
    "    print(\"LOADING AUDIO FILES\")\n",
    "    for i,f in enumerate(tqdm(self.dataset[0])):\n",
    "      # idx_0: patient_id, idx_1: recording_index, idx_2:Chest location, idx_3:A cquistation mode, idx_4: device\n",
    "      tokens = f.strip().split(\"_\")\n",
    "      try:\n",
    "        _,annotation_data = get_annotation_data(f,dataset_dir)\n",
    "        sample_data = get_samples(f,dataset_dir,annotation_data,sample_rate=self.sample_rate)\n",
    "        if tokens[0] not in self.patient_to_samples.keys():\n",
    "          self.patient_to_samples[tokens[0]] = sample_data\n",
    "        else:\n",
    "          self.patient_to_samples[tokens[0]].extend(sample_data)\n",
    "        \n",
    "        if tokens[0] not in self.patient_to_idx.keys():\n",
    "          self.patient_to_idx[tokens[0]] = [i]\n",
    "        else:\n",
    "          self.patient_to_idx[tokens[0]].append(i)\n",
    "        \n",
    "        \n",
    "        self.audio_data.extend(sample_data)\n",
    "        \n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "        failed_files.append(self.dataset.iloc[i,0])\n",
    "        continue\n",
    "\n",
    "  \n",
    "  def augment_audio(self,audio):\n",
    "    effects = [[\"lowpass\", \"-1\", \"300\"],\n",
    "           [\"speed\", \"0.8\"],\n",
    "           [\"rate\", f\"{self.sample_rate}\"],\n",
    "           [\"reverb\", \"-w\"],\n",
    "           [\"channels\", \"1\"],\n",
    "           ]\n",
    "    if self.train_flag:\n",
    "      audio,sr = torchaudio.sox_effects.apply_effects_tensor(audio, self.sample_rate, effects)\n",
    "    # print(audio.shape)\n",
    "    return audio\n",
    "    \n",
    "\n",
    "    # make a dict for easy indexing and loading\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    if torch.is_tensor(index):\n",
    "      index = index.tolist()\n",
    "    audio = self.audio_data[index][0]\n",
    "    # print(audio.shape)\n",
    "    # randomly augment data\n",
    "    # print(\"Augmenting data\")\n",
    "    audio,_ = torchaudio.sox_effects.apply_effects_tensor(audio,self.sample_rate,effects=[[\"channels\",\"1\"]])\n",
    "    if np.random.random() > 0.5:\n",
    "      audio = self.augment_audio(audio)\n",
    "\n",
    "    # pad the audio to desired length using \n",
    "    # print(\"Before padding: \",audio.shape)\n",
    "    if audio.shape[1] < self.desired_length*self.sample_rate:\n",
    "      audio = torch.nn.functional.pad(audio,(0,self.desired_length*self.sample_rate-audio.shape[1]))\n",
    "    else:\n",
    "      audio = audio[:,:self.desired_length*self.sample_rate]\n",
    "    audio_mel_image = torchaudio.transforms.MelSpectrogram(sample_rate=self.sample_rate, n_fft=self.nfft, win_length=self.win_length, hop_length=self.hop, n_mels=self.n_mels, f_max=self.f_max)(audio)\n",
    "    \n",
    "    # blank Region Clipping\n",
    "    audio_mel_image_raw = audio_mel_image.squeeze(0).numpy()\n",
    "    for row in range(audio_mel_image_raw.shape[0]):\n",
    "        black_percent = len(np.where(audio_mel_image_raw[row,:]==-100)[0])/len(audio_mel_image_raw[row,:])\n",
    "        if black_percent > 0.80:\n",
    "            break\n",
    "    audio_mel_image_raw = audio_mel_image_raw[:row+1,:]\n",
    "\n",
    "    for column in range(audio_mel_image_raw.shape[1]):\n",
    "        black_percent = len(np.where(audio_mel_image_raw[:,column]==-100)[0])/len(audio_mel_image_raw[:,column])\n",
    "        if black_percent > 0.90:\n",
    "            break\n",
    "    \n",
    "    audio_mel_image_raw = audio_mel_image_raw[:,:column+1]\n",
    "    audio_mel_image = torch.from_numpy(audio_mel_image_raw).unsqueeze(0)\n",
    "\n",
    "    \n",
    "    label = self.audio_data[index][1]\n",
    "    audio_mel_image = torchvision.transforms.Resize((256,256))(audio_mel_image)\n",
    "  \n",
    "    if self.input_transform is not None:\n",
    "      audio_mel_image = self.input_transform(audio_mel_image)\n",
    "    \n",
    "    label = torch.from_numpy(np.array(label)).float()\n",
    "    return audio_mel_image,label\n",
    "\n",
    "  def __len__(self):\n",
    "    # print(\"Length of dataset: \",len(self.audio_data))\n",
    "    return len(self.audio_data)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_K6QXeG5CgXk"
   },
   "source": [
    "# Implementing Patient Based Tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "class RModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RModel,self).__init__()\n",
    "        self.hidden_dim = 256\n",
    "        self.num_layers = 1\n",
    "        self.input_dim = 128\n",
    "        self.fc_dim = 36864\n",
    "        self.feature_extractor = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm2d(1),\n",
    "            # conv layer with kernel 5x7\n",
    "            torch.nn.Conv2d(1, 64, kernel_size=(5,7)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,3)),\n",
    "            torch.nn.Conv2d(64,64,kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64,64,kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,3)),\n",
    "            torch.nn.Conv2d(64,128,kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,3)),\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128,128,kernel_size=(3,3)),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.fully_connected = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(self.fc_dim,100),\n",
    "            # torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(100,4),\n",
    "            torch.nn.Softmax()\n",
    "        )\n",
    "        self.bi_lstm_block = torch.nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, batch_size,device='cuda'):\n",
    "        return (torch.zeros((self.num_layers * 2, batch_size, self.hidden_dim),device=device),\n",
    "                torch.zeros((self.num_layers * 2, batch_size, self.hidden_dim),device=device))\n",
    "\n",
    "    def init_optimizer(self,learning_rate=0.0001):\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate, amsgrad=False,betas=(0.9, 0.999), weight_decay=5e-4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # reshape the input to (batch_size, num_channels, height, width)\n",
    "        # x = x.permute(0, 3, 2, 1)\n",
    "        x = self.feature_extractor(x)\n",
    "        batch_size, C, H, W = x.size()\n",
    "        x = x.view(batch_size, C, H*W)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        self.fc_dim = self.hidden_dim * H * W * 2\n",
    "        h0, c0 = self.init_hidden_state(batch_size)\n",
    "        x, (hn, cn) = self.bi_lstm_block(x, (h0, c0))\n",
    "        x = torch.nn.Tanh()(x)\n",
    "        x = self.fully_connected(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def save_model(model,name='model.pt'):\n",
    "    if not os.path.isdir('saved_models'):\n",
    "        os.mkdir('saved_models')\n",
    "      \n",
    "    torch.save(model.state_dict(), os.path.join('saved_models', name))\n",
    "    print(\"Model successfully saved.\")\n",
    "    \n",
    "def load_model(model):\n",
    "    model.load_state_dict(torch.load(os.path.join('saved_models', 'model.pt')))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkxQJytR1Sm8",
    "outputId": "00075074-1a38-4070-b1ee-7235ef131100"
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_dir = \"ICBHI_final_database\"\n",
    "train_test_txt = \"ICBHI_challenge_train_test (1).txt\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_df,test_file_df = get_train_test_names(train_test_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageLoader(train_file_df,dataset_dir,train_flag=True)\n",
    "test_dataset = ImageLoader(test_file_df,dataset_dir,train_flag=False) \n",
    "train_dataset,val_dataset = torch.utils.data.random_split(train_dataset,[int(0.8*len(train_dataset)),len(train_dataset)-int(0.8*len(train_dataset))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with audio array, label, start and end\n",
    "train_class_counts = {}\n",
    "for i, (audio,label) in enumerate(train_dataset):\n",
    "    label = int(label)\n",
    "    if label not in train_class_counts.keys():\n",
    "        train_class_counts[label] = 1\n",
    "    else:\n",
    "        train_class_counts[label]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_prob = {\n",
    "    0:train_class_counts[0]/len(train_dataset),\n",
    "    1:train_class_counts[1]/len(train_dataset),\n",
    "    2:train_class_counts[2]/len(train_dataset),\n",
    "    3:train_class_counts[3]/len(train_dataset)\n",
    "}\n",
    "train_class_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_weights = torch.from_numpy(np.array([1/train_class_counts[i] for i in range(4)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_weights=train_class_weights.to(\"cuda\").float()\n",
    "train_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_metrics(preds,targets):\n",
    "    # calculate specificity\n",
    "    # sepcificity = Correct Label[0] + Correct Label[1] + Correct Label[2] + Correct Label[3] / Total Label[0] + Total Label[1] + Total Label[2] + Total Label[3]\n",
    "    sensitivity_num = 0\n",
    "    sensitivity_denm = 0\n",
    "    for i in range(4):\n",
    "        sensitivity_num += torch.sum((preds==i) & (targets==i))\n",
    "        sensitivity_denm += torch.sum(targets==i)\n",
    "    sensitivity = float(sensitivity_num)/float(sensitivity_denm)\n",
    "    specificity = float(torch.sum((preds==0) & (targets==0)))/float(torch.sum(targets==0))\n",
    "    score = 0.5*(sensitivity+specificity)\n",
    "    return sensitivity,specificity,score\n",
    "\n",
    "def get_scores(model, testloader, device):\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_sensitivity, val_specificity, val_score = 0.0, 0.0, 0.0, 0.0\n",
    "        for val_batch in testloader:\n",
    "            imgs, targets = val_batch\n",
    "            imgs, targets = imgs.to(device), targets.to(device).long()\n",
    "            val_outputs = model(imgs)\n",
    "            val_loss += torch.nn.CrossEntropyLoss()(val_outputs, targets).item()\n",
    "            val_preds = torch.argmax(val_outputs, dim=1)\n",
    "            sensitivity,specificity,score = get_evaluation_metrics(val_preds,targets)\n",
    "            val_sensitivity += sensitivity\n",
    "            val_specificity += specificity\n",
    "            val_score += score\n",
    "\n",
    "    val_loss = float(float(val_loss)/float(len(testloader)*BATCH_SIZE))\n",
    "    val_sensitivity = float(float(val_sensitivity)/float(len(testloader)))\n",
    "    val_specificity = float(float(val_specificity)/float(len(testloader)))\n",
    "    val_score = float(float(val_score)/float(len(testloader)))\n",
    "    return val_loss,val_sensitivity,val_specificity,val_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, testloader, device):\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        for val_batch in testloader:\n",
    "            imgs, targets = val_batch\n",
    "            #print(imgs.shape, targets.shape)\n",
    "            #print(targets)\n",
    "            imgs, targets = imgs.to(device), targets.to(device).long()\n",
    "            val_outputs = model(imgs)\n",
    "            val_loss += torch.nn.CrossEntropyLoss()(val_outputs, targets).item()\n",
    "            val_preds = torch.argmax(val_outputs, dim=1)\n",
    "            val_acc += torch.sum(val_preds == targets)\n",
    "\n",
    "    val_acc = float(float(val_acc)/float(len(testloader)*BATCH_SIZE))\n",
    "    return val_acc,val_loss\n",
    "\n",
    "def train_evaluate(model, optimizer, trainloader, valloader, num_epochs=10, save_model_name=None, dropout=None, learning_rate=0.001, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_accuracy_with_epochs = []\n",
    "    val_accuracy_with_epochs = []\n",
    "    loss_with_epochs = []\n",
    "    val_loss_with_epochs = []\n",
    "    # best_val_acc = 0.0\n",
    "    best_loss = 0.0345\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = 0, 0\n",
    "        print(\"\\nEpoch: \", str(epoch+1), \"/\", str(num_epochs))\n",
    "\n",
    "        with tqdm(total=len(trainloader)) as pbar:\n",
    "            for idx, batch in enumerate(trainloader):\n",
    "                images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device).long()\n",
    "                preds = model(images)\n",
    "                loss = torch.nn.CrossEntropyLoss(weight=train_class_weights)(preds, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print(torch.argmax(preds, dim=1),labels)\n",
    "                train_loss += loss.item()\n",
    "                acc = torch.sum(torch.argmax(preds, dim=1) == labels)\n",
    "                train_acc += acc\n",
    "                pbar.set_postfix(Loss='{0:.4f}'.format(loss.item()), Accuracy='{0:.4f}'.format(float(train_acc.item()/(BATCH_SIZE*(idx+1)))))\n",
    "                pbar.update(1)\n",
    "\n",
    "            val_acc, val_loss = evaluate(model, valloader, device)\n",
    "            val_accuracy_with_epochs.append(val_acc)\n",
    "            val_loss_with_epochs.append(val_loss)\n",
    "            loss_with_epochs.append(train_loss)\n",
    "            train_loss = float(float(train_loss)/float(len(trainloader)*BATCH_SIZE))\n",
    "            print(\"train_acc:\", round(float(float(train_acc)/float(len(trainloader)*BATCH_SIZE)), 4), \" val_acc:\", round(val_acc, 4))\n",
    "            train_accuracy_with_epochs.append(round(float(float(train_acc)/float(len(trainloader)*BATCH_SIZE)), 4))\n",
    "            \n",
    "            if train_loss <= best_loss:\n",
    "                best_loss = train_loss\n",
    "                if save_model_name is not None:\n",
    "                    torch.save(model.state_dict(), save_model_name)\n",
    "                    print(\"Model saved at\", save_model_name)\n",
    "                    print(\"Best train loss:\", best_loss)\n",
    "\n",
    "    return train_accuracy_with_epochs, val_accuracy_with_epochs, model, loss_with_epochs, val_loss_with_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Mobile NET \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights)\n",
    "new_conv = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "# with torch.no_grad():\n",
    "#     new_conv.weight[:, 0, :, :] = mobilenet_model.features[0].weight.sum(dim=1)\n",
    "mobilenet_model.features[0] = new_conv\n",
    "mobilenet_in_features = mobilenet_model.classifier[1].in_features\n",
    "mobilenet_model.classifier[1] = torch.nn.Linear(mobilenet_in_features,4)\n",
    "\n",
    "mobile_optimizer = torch.optim.Adam(mobilenet_model.parameters(),lr=0.0001,betas=(0.9,0.99),amsgrad=False)\n",
    "mobilenet_train_acc,mobilenet_val_acc,mobilenet_model,mobilenet_loss_with_epochs, mobilenet_val_loss_with_epochs = train_evaluate(mobilenet_model,mobile_optimizer,train_loader,val_loader,num_epochs=50,save_model_name=\"mobilenetmodel.pt\",dropout=0.5,learning_rate=0.0001,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "plt.plot(mobilenet_val_acc,label=\"Validation\")\n",
    "plt.plot(mobilenet_train_acc,label=\"Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Accuracy for Mobilenet backbone\")\n",
    "plt.savefig(\"plots/mobilenet_train_vs_validation_acc.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(mobilenet_loss_with_epochs,label=\"Train\")\n",
    "plt.plot(mobilenet_val_loss_with_epochs,label=\"Validation\")\n",
    "plt.title(\"MobileNet-Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/mobilenet_train_vs_val_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "vgg16_model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "vgg16_model.features[0]=torch.nn.Conv2d(1,64,kernel_size=3,padding=1,stride=1)\n",
    "vgg_in_features = vgg16_model.classifier[6].in_features\n",
    "vgg16_model.classifier[6] = torch.nn.Linear(vgg_in_features,4)\n",
    "vgg_optimizer = torch.optim.Adam(vgg16_model.parameters(),lr=0.0001,betas=(0.9,0.99),amsgrad=False)\n",
    "vgg16_train_acc,vgg16_val_acc,vgg16_model,vgg16_loss_with_epochs,vgg16_val_loss_with_epochs = train_evaluate(vgg16_model,vgg_optimizer,train_loader,val_loader,num_epochs=50,save_model_name=\"vgg16model.pt\",dropout=0.5,learning_rate=0.0001,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(vgg16_val_acc,label=\"Validation\")\n",
    "plt.plot(vgg16_train_acc,label=\"Train\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Accuracy for Vgg16 backbone\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(\"plots/Vgg16_train_vs_validation_acc.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(vgg16_loss_with_epochs,label=\"Train\")\n",
    "plt.plot(vgg16_val_loss_with_epochs,label=\"Validation\")\n",
    "plt.title(\"Vgg16-Loss Curve\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/Vgg16_train_vs_val_loss.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Hybrid CNN-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkgEptHZkkx6"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.99,0.99),amsgrad=False)\n",
    "train_acc,val_acc,model,loss_with_epochs,val_loss_with_epochs = train_evaluate(model,optimizer,train_loader,val_loader,num_epochs=50,save_model_name=\"model.pt\",dropout=0.5,learning_rate=0.0001,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "plt.figure()\n",
    "plt.plot(val_acc,label=\"Validation\")\n",
    "plt.plot(train_acc,label=\"Train\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Accuracy for Patient Based Model\")\n",
    "plt.savefig(\"plots/patient_based_model_train_vs_validation_acc.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_with_epochs,label=\"Train\")\n",
    "plt.plot(val_loss_with_epochs,label=\"Validation\")\n",
    "plt.title(\"Patient Based Model-Loss Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/patient_based_model_train_vs_val_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune the patient based model for a particular patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_wise_tune_data = train_dataset.dataset.patient_to_idx\n",
    "# patient to samples is a dict containing patient id as key and list of samples as value\n",
    "# we need to fine tune the model patient wise\n",
    "# we will use the same model as before\n",
    "\n",
    "patient_id = \"107\"\n",
    "patient_samples = patient_wise_tune_data[patient_id]\n",
    "patient_train_samples = patient_samples[:int(0.8*len(patient_samples))]\n",
    "patient_test_samples = patient_samples[int(0.8*len(patient_samples)):]\n",
    "patient_train_dataset = torch.utils.data.Subset(train_dataset,patient_train_samples)\n",
    "patient_test_dataset = torch.utils.data.Subset(train_dataset,patient_test_samples)\n",
    "patient_train_loader = torch.utils.data.DataLoader(patient_train_dataset,batch_size=32,shuffle=True)\n",
    "patient_test_loader = torch.utils.data.DataLoader(patient_test_dataset,batch_size=32,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_optimizer = torch.optim.Adam(model.parameters(),lr=0.0001,betas=(0.9,0.99),amsgrad=False,weight_decay=0.0001)\n",
    "fine_tune_train_acc,fine_tune_val_acc,fine_tune_model,fine_tune_loss_with_epochs,fine_tune_val_loss_with_epochs = train_evaluate(model,fine_tune_optimizer,patient_train_loader,patient_test_loader,num_epochs=200,save_model_name=\"fine_tune_model.pt\",dropout=0.5,learning_rate=0.0001,device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved models for direct testing\n",
    "# fine_tune_model = RModel()\n",
    "# fine_tune_model.load_state_dict(torch.load(\"saved_models/fine_tune_model.pt\"))\n",
    "# fine_tune_model = fine_tune_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved models for direct testing\n",
    "# vgg16_model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.DEFAULT)\n",
    "# vgg16_model.features[0]=torch.nn.Conv2d(1,64,kernel_size=3,padding=1,stride=1)\n",
    "# vgg_in_features = vgg16_model.classifier[6].in_features\n",
    "# vgg16_model.classifier[6] = torch.nn.Linear(vgg_in_features,4)\n",
    "\n",
    "# vgg16_model.load_state_dict(torch.load(\"saved_models/vgg16model.pt\"))\n",
    "# vgg16_model = vgg16_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved models for direct testing\n",
    "# mobilenet_model = torchvision.models.mobilenet_v2(weights=torchvision.models.MobileNet_V2_Weights)\n",
    "# new_conv = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "# # with torch.no_grad():\n",
    "# #     new_conv.weight[:, 0, :, :] = mobilenet_model.features[0].weight.sum(dim=1)\n",
    "# mobilenet_model.features[0] = new_conv\n",
    "# mobilenet_in_features = mobilenet_model.classifier[1].in_features\n",
    "# mobilenet_model.classifier[1] = torch.nn.Linear(mobilenet_in_features,4)\n",
    "# mobilenet_model.load_state_dict(torch.load(\"saved_models/mobilenetmodel.pt\"))\n",
    "# mobilenet_model = mobilenet_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results on Testing Data\")\n",
    "loss,specificity, sensitivity, score = get_scores(fine_tune_model, test_loader, device)\n",
    "print(\"Test Loss on fine tuned patient specific model: \",loss)\n",
    "print(\"Test Specificity on fine tuned patient specific model: \",specificity*100)\n",
    "print(\"Test Sensitivity on fine tuned patient specific model: \",sensitivity*100)\n",
    "print(\"Test Score on fine tuned patient specific model: \",score*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results on Testing Data\")\n",
    "loss,specificity, sensitivity, score = get_scores(vgg16_model, test_loader, device)\n",
    "print(\"Test Loss on fine tuned Vgg16 backbone model: \",loss)\n",
    "print(\"Test Specificity on fine tuned Vgg16 backbone model: \",specificity)\n",
    "print(\"Test Sensitivity on fine tuned Vgg16 backbone model: \",sensitivity)\n",
    "print(\"Test Score on fine tuned Vgg16 backbone model: \",score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results on Testing Data\")\n",
    "loss,specificity, sensitivity, score = get_scores(mobilenet_model, test_loader, device)\n",
    "print(\"Test Loss on fine tuned Mobilenet backbone model: \",loss)\n",
    "print(\"Test Specificity on fine tuned Mobilenet backbone model: \",specificity)\n",
    "print(\"Test Sensitivity on fine tuned Mobilenet backbone model: \",sensitivity)\n",
    "print(\"Test Score on fine tuned Mobilenet backbone model: \",score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results on patient 107 using fine tuned patient specific model\")\n",
    "loss,specificity, sensitivity, score = get_scores(fine_tune_model, patient_test_loader, device)\n",
    "print(\"Test Loss for patient 107 on fine tuned patient specific model: \",loss)\n",
    "print(\"Test Specificity for patient 107 on fine tuned patient specific model: \",specificity*100)\n",
    "print(\"Test Sensitivity for patient 107 on fine tuned patient specific model: \",sensitivity*100)\n",
    "print(\"Test Score for patient 107 on fine tuned patient specific model: \",score*100)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Results on patient 107 using fine tuned Vgg16 backbone model\")\n",
    "loss,specificity, sensitivity, score = get_scores(vgg16_model, patient_test_loader, device)\n",
    "print(\"Test Loss for patient 107 on fine tuned Vgg16 backbone model: \",loss)\n",
    "print(\"Test Specificity for patient 107 on fine tuned Vgg16 backbone model: \",specificity*100)\n",
    "print(\"Test Sensitivity for patient 107 on fine tuned Vgg16 backbone model: \",sensitivity*100)\n",
    "print(\"Test Score for patient 107 on fine tuned Vgg16 backbone model: \",score*100)\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Results on patient 107 using fine tuned Mobilenet backbone model\")\n",
    "loss,specificity, sensitivity, score = get_scores(mobilenet_model, patient_test_loader, device)\n",
    "print(\"Test Loss for patient 107 on fine tuned Mobilenet backbone model: \",loss)\n",
    "print(\"Test Specificity for patient 107 on fine tuned Mobilenet backbone model: \",specificity*100)\n",
    "print(\"Test Sensitivity for patient 107 on fine tuned Mobilenet backbone model: \",sensitivity*100)\n",
    "print(\"Test Score for patient 107 on fine tuned Mobilenet backbone model: \",score*100)\n",
    "print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model summary\n",
    "import torchsummary\n",
    "print(\"Patient Based Model Summary\")\n",
    "torchsummary.summary(fine_tune_model,(1,256,256))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MobileNet Model Summary\")\n",
    "torchsummary.summary(mobilenet_model,(1,256,256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vgg16 Model Summary\")\n",
    "torchsummary.summary(vgg16_model,(1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
